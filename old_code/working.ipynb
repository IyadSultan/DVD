{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the LLM\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define data models\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMCQ\u001b[39;00m(BaseModel):\n",
      "File \u001b[1;32md:\\Dropbox\\AI\\Projects\\DVD\\.venv\\lib\\site-packages\\langchain_core\\load\\serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Dropbox\\AI\\Projects\\DVD\\.venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:529\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[0;32m    528\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[1;32md:\\Dropbox\\AI\\Projects\\DVD\\.venv\\lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Define data models\n",
    "class MCQ(BaseModel):\n",
    "    question: str\n",
    "    options: List[str]\n",
    "    correct_answer: str\n",
    "    relevance_score: float = 0.0  # Add relevance score to each MCQ\n",
    "\n",
    "class Document(BaseModel):\n",
    "    content: str\n",
    "    mcqs: List[MCQ] = Field(default_factory=list)\n",
    "\n",
    "class DVDState(BaseModel):\n",
    "    documents: List[Document] = Field(default_factory=list)\n",
    "    current_document_index: int = 0\n",
    "    mcqs_generated: bool = False\n",
    "    mcqs_evaluated: bool = False\n",
    "    user_responses: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    evaluation_results: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "# Function to load document content from file\n",
    "def load_document(filename: str) -> str:\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return \"\"\n",
    "    except IOError:\n",
    "        print(f\"Error: Unable to read file '{filename}'.\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to generate MCQs\n",
    "def generate_mcqs(state: DVDState) -> DVDState:\n",
    "    current_doc = state.documents[state.current_document_index]\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "    You are an expert in creating challenging multiple-choice questions (MCQs) based on medical notes. \n",
    "    Generate 10 MCQs that are directly related to the content of the given medical note. \n",
    "    Each MCQ should have 5 answer choices (A, B, C, D, E), including \"I don't know\" as the last option.\n",
    "    Ensure that the questions are not obvious and require a good factual grasp of the note's content.\n",
    "    Format each MCQ as follows exactly with no additional text or formatting:\n",
    "    Question: [Question text]\n",
    "    A. [Option A]\n",
    "    B. [Option B]\n",
    "    C. [Option C]\n",
    "    D. [Option D]\n",
    "    E. I don't know\n",
    "    Correct Answer: [Correct option letter]\n",
    "    \"\"\"\n",
    "    \n",
    "    human_message = f\"Generate 10 MCQs based on this medical note:\\n\\n{current_doc.content}\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=human_message)\n",
    "    ])\n",
    "    print(f\"Generating MCQs for document {state.current_document_index + 1}\")\n",
    "    print(response.content)\n",
    "    # Parse the response and create MCQ objects\n",
    "    mcqs = []\n",
    "    for mcq_text in response.content.split(\"\\n\\n\"):\n",
    "        lines = [line.strip() for line in mcq_text.split(\"\\n\") if line.strip()]\n",
    "        if len(lines) < 7:\n",
    "            continue  # Skip incomplete MCQs\n",
    "        \n",
    "        question = lines[0].replace(\"Question: \", \"\").strip()\n",
    "        options = [line.split(\". \", 1)[1].strip() for line in lines[1:6] if \". \" in line]\n",
    "        correct_answer_line = next((line for line in lines if line.lower().startswith(\"correct answer:\")), None)\n",
    "        print(f\"correct_answer_line: {correct_answer_line}\")\n",
    "        \n",
    "        if correct_answer_line and len(options) == 5:\n",
    "            correct_answer_letter = correct_answer_line.split(\":\", 1)[1].strip()\n",
    "            correct_answer_index = ord(correct_answer_letter.upper()) - ord('A')\n",
    "            if 0 <= correct_answer_index < len(options):\n",
    "                correct_answer = options[correct_answer_index]\n",
    "            else:\n",
    "                correct_answer = options[-1]  # Default to last option if index is invalid\n",
    "            mcqs.append(MCQ(question=question, options=options, correct_answer=correct_answer))\n",
    "            print(mcqs[-1])\n",
    "    \n",
    "    new_state = DVDState(**state.model_dump())\n",
    "    new_state.documents[state.current_document_index].mcqs = mcqs\n",
    "    new_state.current_document_index += 1\n",
    "    \n",
    "    if new_state.current_document_index >= len(new_state.documents):\n",
    "        new_state.mcqs_generated = True\n",
    "        new_state.current_document_index = 0  # Reset for next steps\n",
    "        \n",
    "        # Combine MCQs from both documents and limit to 20\n",
    "        all_mcqs = []\n",
    "        for doc in new_state.documents:\n",
    "            all_mcqs.extend(doc.mcqs)\n",
    "        combined_mcqs = all_mcqs[:20]\n",
    "        \n",
    "        # Update both documents with the combined MCQs\n",
    "        for doc in new_state.documents:\n",
    "            doc.mcqs = combined_mcqs\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "\n",
    "# Function to evaluate MCQ relevance\n",
    "def evaluate_mcq_relevance(state: DVDState) -> DVDState:\n",
    "    print(\"Evaluating MCQ relevance\")\n",
    "    \n",
    "    new_state = DVDState(**state.model_dump())\n",
    "    \n",
    "    # for doc in new_state.documents:\n",
    "    #     for mcq in doc.mcqs:\n",
    "    #         # Prepare a prompt to evaluate relevance\n",
    "    #         relevance_prompt = f\"Evaluate the relevance of the following MCQ to the medical history.  Provide a relevance score between 0 and 1.  Higher score means kind of information that is clinically relevant and should be present in a medical note.\\n\\nMCQ:\\nQuestion: {mcq.question}\\nA. {mcq.options[0]}\\nB. {mcq.options[1]}\\nC. {mcq.options[2]}\\nD. {mcq.options[3]}\\nE. I don't know\"\n",
    "            \n",
    "    #         response = llm.invoke([HumanMessage(content=relevance_prompt)])\n",
    "    #         try:\n",
    "    #             relevance_score = float(response.content.strip())\n",
    "    #         except ValueError:\n",
    "    #             relevance_score = 0.0  # Default to 0 if parsing fails\n",
    "            \n",
    "    #         mcq.relevance_score = relevance_score\n",
    "    \n",
    "    new_state.mcqs_evaluated = True\n",
    "    return new_state\n",
    "\n",
    "# Function to present MCQs and collect automated responses\n",
    "def present_mcqs(state: DVDState) -> DVDState:\n",
    "    print(\"Presenting MCQs and collecting automated responses\")\n",
    "    \n",
    "    user_responses = []\n",
    "    for doc in state.documents:\n",
    "        for mcq in doc.mcqs:\n",
    "            # Create the prompt for answering each question\n",
    "            answer_prompt = f\"\"\"\n",
    "            You are given the following multiple-choice question based on the provided document content. Provide the best answer based on the given choices.\n",
    "            Document Content: {doc.content}\n",
    "            Question: {mcq.question}\n",
    "            A. {mcq.options[0]}\n",
    "            B. {mcq.options[1]}\n",
    "            C. {mcq.options[2]}\n",
    "            D. {mcq.options[3]}\n",
    "            E. I don't know\n",
    "            Respond with the option letter (A, B, C, D, or E) that you think is the correct answer.\n",
    "            Do not include any other text in your response.\n",
    "            \"\"\"\n",
    "            print(answer_prompt)\n",
    "            response = llm.invoke([HumanMessage(content=answer_prompt)])\n",
    "            answer = response.content.strip()\n",
    "            print(f\"answer: {answer}\")\n",
    "\n",
    "            # Validate the response is one of the expected options\n",
    "            if answer not in ['A', 'B', 'C', 'D', 'E']:\n",
    "                answer = 'E'  # Default to \"I don't know\" if there's any issue\n",
    "            \n",
    "            user_responses.append({\n",
    "                \"document\": doc,\n",
    "                \"question\": mcq.question,\n",
    "                \"user_answer\": mcq.options[ord(answer) - ord('A')],\n",
    "                \"correct_answer\": mcq.correct_answer,\n",
    "                \"relevance_score\": mcq.relevance_score\n",
    "            })\n",
    "    \n",
    "    new_state = DVDState(**state.model_dump())\n",
    "    new_state.user_responses = user_responses\n",
    "    return new_state\n",
    "\n",
    "# Function to evaluate user responses\n",
    "def evaluate_responses(state: DVDState) -> DVDState:\n",
    "    print(\"Evaluating responses\")\n",
    "    results = {\n",
    "        \"document1\": {\"correct\": 0, \"wrong\": 0, \"unknown\": 0},\n",
    "        \"document2\": {\"correct\": 0, \"wrong\": 0, \"unknown\": 0}\n",
    "    }\n",
    "    \n",
    "    # Create a new list to store valid responses\n",
    "    valid_responses = []\n",
    "    question_responses = {}\n",
    "    \n",
    "    for response in state.user_responses:\n",
    "        doc_index = \"document1\" if state.documents.index(response[\"document\"]) == 0 else \"document2\"\n",
    "        question = response[\"question\"]\n",
    "        \n",
    "        if question not in question_responses:\n",
    "            question_responses[question] = {\"document1\": None, \"document2\": None}\n",
    "        \n",
    "        question_responses[question][doc_index] = response\n",
    "    \n",
    "    for question, docs_responses in question_responses.items():\n",
    "        doc1_response = docs_responses[\"document1\"]\n",
    "        doc2_response = docs_responses[\"document2\"]\n",
    "        \n",
    "        if (doc1_response[\"user_answer\"] != doc1_response[\"correct_answer\"] and\n",
    "            doc2_response[\"user_answer\"] != doc2_response[\"correct_answer\"]):\n",
    "            # Both documents got the answer wrong, so we skip this question\n",
    "            continue\n",
    "        \n",
    "        for doc_index, response in docs_responses.items():\n",
    "            if response[\"user_answer\"] == response[\"correct_answer\"]:\n",
    "                # If the answer is correct, keep it and update the results\n",
    "                results[doc_index][\"correct\"] += 1\n",
    "                valid_responses.append(response)\n",
    "            elif response[\"user_answer\"] == \"I don't know\":\n",
    "                # If the answer is \"I don't know\", keep it and update the results\n",
    "                results[doc_index][\"unknown\"] += 1\n",
    "                valid_responses.append(response)\n",
    "            else:\n",
    "                # If the answer is wrong, don't include it in valid_responses\n",
    "                # We don't update results for wrong answers as they're being removed\n",
    "                pass\n",
    "    \n",
    "    # Replace the original user_responses with the filtered valid_responses\n",
    "    new_state = DVDState(**state.model_dump())\n",
    "    new_state.user_responses = valid_responses\n",
    "    print(\"Evaluating responses\")\n",
    "    results = {\n",
    "        \"document1\": {\"correct\": 0, \"wrong\": 0, \"unknown\": 0},\n",
    "        \"document2\": {\"correct\": 0, \"wrong\": 0, \"unknown\": 0}\n",
    "    }\n",
    "    \n",
    "    for response in state.user_responses:\n",
    "        doc_index = \"document1\" if state.documents.index(response[\"document\"]) == 0 else \"document2\"\n",
    "        if response[\"user_answer\"] == \"I don't know\":\n",
    "            results[doc_index][\"unknown\"] += 1\n",
    "        elif response[\"user_answer\"] == response[\"correct_answer\"]:\n",
    "            results[doc_index][\"correct\"] += 1\n",
    "        else:\n",
    "            results[doc_index][\"wrong\"] += 1\n",
    "    \n",
    "    # Print final scores for both documents and detailed MCQ information\n",
    "    print(\"\\nFinal Scores:\")\n",
    "    for doc, scores in results.items():\n",
    "        total_questions = scores['correct'] + scores['wrong'] + scores['unknown']\n",
    "        print(f\"\\n{doc.capitalize()}:\")\n",
    "        print(f\"  Total Questions: {total_questions}\")\n",
    "        print(f\"  Correct: {scores['correct']}\")\n",
    "        print(f\"  Wrong: {scores['wrong']}\")\n",
    "        print(f\"  Unknown: {scores['unknown']}\")\n",
    "    \n",
    "    # Print MCQs, relevance scores, and answers of both models\n",
    "    print(\"\\nDetailed MCQ Evaluation:\")\n",
    "    for response in state.user_responses:\n",
    "        print(f\"\\nQuestion: {response['question']}\")\n",
    "        print(f\"Relevance Score: {response['relevance_score']}\")\n",
    "        print(f\"Correct Answer: {response['correct_answer']}\")\n",
    "        print(f\"Model Answer: {response['user_answer']}\")\n",
    "    \n",
    "    new_state = DVDState(**state.model_dump())\n",
    "    new_state.evaluation_results = results\n",
    "    return new_state\n",
    "\n",
    "# Build the state graph\n",
    "def build_dvd_graph() -> StateGraph:\n",
    "    # make sure DVDState is empty before starting\n",
    "    DVDState.model_validate({})\n",
    "\n",
    "    workflow = StateGraph(DVDState)\n",
    "    \n",
    "    workflow.add_node(\"generate_mcqs\", generate_mcqs)\n",
    "    workflow.add_node(\"evaluate_mcq_relevance\", evaluate_mcq_relevance)\n",
    "    workflow.add_node(\"present_mcqs\", present_mcqs)\n",
    "    workflow.add_node(\"evaluate_responses\", evaluate_responses)\n",
    "    \n",
    "    workflow.set_entry_point(\"generate_mcqs\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_mcqs\",\n",
    "        lambda x: \"generate_mcqs\" if not x.mcqs_generated else \"evaluate_mcq_relevance\"\n",
    "    )\n",
    "    workflow.add_edge(\"evaluate_mcq_relevance\", \"present_mcqs\")\n",
    "    # workflow.add_edge(\"generate_mcqs\", \"present_mcqs\")\n",
    "    workflow.add_edge(\"present_mcqs\", \"evaluate_responses\")\n",
    "    workflow.add_edge(\"evaluate_responses\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Run the DVD evaluation\n",
    "def run_dvd_evaluation(doc1_filename: str, doc2_filename: str):\n",
    "    doc1_content = load_document(doc1_filename)\n",
    "    doc2_content = load_document(doc2_filename)\n",
    "\n",
    "    if not doc1_content or not doc2_content:\n",
    "        print(\"Error: Unable to load one or both documents. Exiting.\")\n",
    "        return\n",
    "\n",
    "    initial_state = DVDState(documents=[\n",
    "        Document(content=doc1_content),\n",
    "        Document(content=doc2_content)\n",
    "    ])\n",
    "    graph = build_dvd_graph()\n",
    "    \n",
    "    \n",
    "    for event in graph.stream(initial_state, {\"configurable\": {\"thread_id\": \"dvd_evaluation\"}}):\n",
    "        if isinstance(event, DVDState) and event.evaluation_results:\n",
    "            results = event.evaluation_results\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for doc, scores in results.items():\n",
    "                print(f\"\\n{doc.capitalize()}:\")\n",
    "                print(f\"  Correct: {scores['correct']}\")\n",
    "                print(f\"  Wrong: {scores['wrong']}\")\n",
    "                print(f\"  Unknown: {scores['unknown']}\")\n",
    "\n",
    "# Run the evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    run_dvd_evaluation(\"human.txt\", \"ai.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
